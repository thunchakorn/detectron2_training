{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d7186a9a76fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2_imshow\u001b[0m \u001b[1;31m# On your local machine you don't need it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import logging\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.engine import DefaultPredictor, default_argument_parser, default_setup, launch\n",
    "from detectron2 import config\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.data.datasets import load_coco_json, register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    "    get_detection_dataset_dicts,\n",
    "    DatasetFromList,\n",
    "    DatasetMapper,\n",
    "    MapDataset\n",
    ")\n",
    "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "from detectron2.utils.events import (\n",
    "    CommonMetricPrinter,\n",
    "    EventStorage,\n",
    "    JSONWriter,\n",
    "    TensorboardXWriter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regist_dataset(json_train_path, json_test_path):\n",
    "    \"\"\"\n",
    "    register training and testing dataset\n",
    "    dataset must be in coco format\n",
    "    ouput: name of training and testing set\n",
    "    \"\"\"\n",
    "    train_name = os.path.split(json_train_path)[-1][:-5]\n",
    "    test_name = os.path.split(json_test_path)[-1][:-5]\n",
    "    register_coco_instances(train_name,\n",
    "                            {},\n",
    "                            json_train_path,\n",
    "                            \"\")\n",
    "    \n",
    "    register_coco_instances(test_name,\n",
    "                            {},\n",
    "                            json_test_path,\n",
    "                            \"\")\n",
    "    return train_name, test_name\n",
    "\n",
    "def setup_hyperparameter(config_model_zoo, **kwargs):\n",
    "  \"\"\"\n",
    "  Setup hyperparameter in config for training \n",
    "  Input:\n",
    "    config_model_zoo:\n",
    "      cofig from model zoo\n",
    "    **kwargs:\n",
    "      other configs, ref: https://detectron2.readthedocs.io/modules/config.html#config-references\n",
    "      change period (.) to double-underscore (__) eg. MODEL.WEIGHTS -> MODEL__WEIGHTS\n",
    "      MODEL.ROI_HEADS.SCORE_THRESH_TEST -> MODEL__ROI_HEADS__SCORE_THRES_TEST\n",
    "  Return:\n",
    "    cfg file:\n",
    "    All keyword hyperparameter for logging\n",
    "  \"\"\"\n",
    "\n",
    "  parser = default_argument_parser()\n",
    "  parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  cfg = get_cfg()\n",
    "  cfg.merge_from_file(config_model_zoo)\n",
    "  hyper_parameters = {}\n",
    "  for key, value in kwargs.items():\n",
    "    key_param = key.replace('__', '.')\n",
    "    cfg.merge_from_list([key_param, value])\n",
    "    hyper_parameters[key_param] = value # log hyperparameter\n",
    "  default_setup(cfg, args)\n",
    "  return cfg, hyper_parameters\n",
    "\n",
    "\n",
    "def do_evaluate(cfg, model):\n",
    "    \"\"\"\n",
    "    Evaluate on test set using coco evaluate\n",
    "    \"\"\"\n",
    "    results = OrderedDict()\n",
    "    for dataset_name in cfg.DATASETS.TEST:\n",
    "        data_loader = build_detection_test_loader(cfg, dataset_name)\n",
    "        evaluator = COCOEvaluator(dataset_name, cfg, False, output_dir= cfg.OUTPUT_DIR)\n",
    "        results_i = inference_on_dataset(model, data_loader, evaluator)\n",
    "        results[dataset_name] = results_i\n",
    "        if comm.is_main_process():\n",
    "            logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n",
    "            print_csv_format(results_i)\n",
    "    if len(results) == 1:\n",
    "        results = list(results.values())[0]\n",
    "    return results\n",
    "\n",
    "def val_mapper(dataset_dict):\n",
    "  mapper = DatasetMapper(cfg, True)\n",
    "  return mapper(dataset_dict)\n",
    "\n",
    "def do_val(cfg, model, data_val_loader, storage):\n",
    "  total_val_loss = []\n",
    "  with torch.no_grad():\n",
    "    for data in data_val_loader:\n",
    "      loss_dict = model(data)\n",
    "      losses = sum(loss_dict.values())\n",
    "      total_val_loss.append(losses)\n",
    "  return sum(total_val_loss)/len(total_val_loss)\n",
    "\n",
    "\n",
    "def do_train(cfg, model, resume=False):\n",
    "    model.train()\n",
    "    optimizer = build_optimizer(cfg, model)\n",
    "    scheduler = build_lr_scheduler(cfg, optimizer)\n",
    "\n",
    "    checkpointer = DetectionCheckpointer(\n",
    "        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler\n",
    "    )\n",
    "    start_iter = (\n",
    "        checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1\n",
    "    )\n",
    "    max_iter = cfg.SOLVER.MAX_ITER\n",
    "\n",
    "    # periodic_checkpointer = PeriodicCheckpointer(\n",
    "    #     checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter\n",
    "    # )\n",
    "\n",
    "    writers = (\n",
    "        [\n",
    "            CommonMetricPrinter(max_iter),\n",
    "            JSONWriter(os.path.join(cfg.OUTPUT_DIR, \"metrics.json\")),\n",
    "            TensorboardXWriter(cfg.OUTPUT_DIR),\n",
    "        ]\n",
    "        if comm.is_main_process()\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    # compared to \"train_net.py\", we do not support accurate timing and\n",
    "    # precise BN here, because they are not trivial to implement\n",
    "    data_loader = build_detection_train_loader(cfg)\n",
    "    \n",
    "    best_val_loss = None\n",
    "    data_val_loader = build_detection_test_loader(cfg,\n",
    "                                                  cfg.DATASETS.TEST[0],\n",
    "                                                  mapper = val_mapper)\n",
    "    logger.info(\"Starting training from iteration {}\".format(start_iter))\n",
    "\n",
    "    with EventStorage(start_iter) as storage:\n",
    "        for data, iteration in zip(data_loader, range(start_iter, max_iter)):\n",
    "            iteration += 1\n",
    "            start = time.time()\n",
    "            storage.step()\n",
    "\n",
    "            loss_dict = model(data)\n",
    "            losses = sum(loss_dict.values())\n",
    "            assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
    "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "            if comm.is_main_process():\n",
    "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
    "            scheduler.step()\n",
    "\n",
    "            if (\n",
    "                cfg.TEST.EVAL_PERIOD > 0\n",
    "                and iteration % cfg.TEST.EVAL_PERIOD == 0\n",
    "                and iteration != max_iter\n",
    "            ):\n",
    "                logger.setLevel(logging.CRITICAL)\n",
    "                print('validating')\n",
    "                val_total_loss = do_val(cfg, model, data_val_loader, storage)\n",
    "                logger.setLevel(logging.DEBUG)\n",
    "                logger.info(f\"validation loss of iteration {iteration}th: {val_total_loss}\")\n",
    "                \n",
    "                if best_val_loss is None or val_total_loss < best_val_loss:\n",
    "                  best_val_loss = copy.deepcopy(val_total_loss)\n",
    "                  best_model_weight = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                comm.synchronize()\n",
    "            \n",
    "            # สร้าง checkpointer เพิ่มให้ save best model โดยดูจาก val loss\n",
    "            if iteration - start_iter > 5 and (iteration % 20 == 0 or iteration == max_iter):\n",
    "                logger.info(f\"time per iteration: {(time.time() - start)/20} seconds\")\n",
    "                for writer in writers:\n",
    "                    writer.write()\n",
    "            \n",
    "    model.load_state_dict(best_model_weight)\n",
    "    checkpointer.save('model_best')     \n",
    "    return model\n",
    "            # periodic_checkpointer.step(iteration - 1) \n",
    "\n",
    "\n",
    "def compare_gt(dataset_list_dict, cfg, dataset_name, WEIGHTS, score_thres_test = 0.7, num_sample = 10):\n",
    "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "  cfg.MODEL.WEIGHTS = WEIGHTS\n",
    "  predictor = DefaultPredictor(cfg)\n",
    "\n",
    "  if not os.path.isdir('compare_result'):\n",
    "    os.mkdir('compare_result')\n",
    "  \n",
    "  if len(dataset_list_dict) > num_sample:\n",
    "    sample = random.sample(range(len(dataset_list_dict)), num_sample)\n",
    "  else:\n",
    "    sample = range(len(dataset_list_dict))\n",
    "  for s in sample:\n",
    "    img_dict = dataset_list_dict[s]\n",
    "    print(img_dict['file_name'])\n",
    "    img = read_image(img_dict['file_name'],format = 'BGR')\n",
    "    h, w = img_dict['height'], img_dict['width']\n",
    "    v_gt = Visualizer(img[:, :, ::-1],\n",
    "                            metadata=MetadataCatalog.get(dataset_name),\n",
    "                            scale=0.5)\n",
    "    v_gt = v_gt.draw_dataset_dict(img_dict)\n",
    "\n",
    "    #predicting\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    #visualizing frmo prediction result\n",
    "    \n",
    "    v_pd = Visualizer(img[:, :, ::-1], MetadataCatalog.get(dataset_name), scale=1.2)\n",
    "    v_pd = v_pd.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    gt = cv2.resize(v_gt.get_image()[:, :, ::-1], (w,h))\n",
    "    pd = cv2.resize(v_pd.get_image()[:, :, ::-1], (w,h))\n",
    "\n",
    "    #stacking groudtruth and prediction\n",
    "    merge_img = np.hstack((gt, pd))\n",
    "    result_name = os.path.join('compare_result/', os.path.split(d['file_name'])[1])\n",
    "    cv2.imwrite(result_name, merge_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"detectron2\")\n",
    "\n",
    "cfg, hyper_parameters = setup_hyperparameter(model_zoo.get_config_file(\n",
    "                        \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"),\n",
    "                        MODEL__WEIGHTS = 'model_final_trimmed.pth',\n",
    "                        MODEL__ROI_HEADS__BATCH_SIZE_PER_IMAGE = 128,\n",
    "                        MODEL__ROI_HEADS__NUM_CLASSES = len(MetadataCatalog.get(val_name).thing_classes),\n",
    "                        DATASETS__TRAIN = (train_name, ),\n",
    "                        DATASETS__TEST = (val_name, ),\n",
    "                        TEST__EVAL_PERIOD = 100,\n",
    "                        DATALOADER__NUM_WORKERS = 2,\n",
    "                        SOLVER__IMS_PER_BATCH = 2,\n",
    "                        SOLVER__BASE_LR = 0.001,\n",
    "                        SOLVER__MAX_ITER = 2000,\n",
    "                        SOLVER__WARMUP_ITERS = 1000,\n",
    "                        SOLVER__STEPS = (1500,),\n",
    "                        SOLVER__GAMMA = 0.1\n",
    "                        )\n",
    "\n",
    "model = build_model(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log hyperparameter\n",
    "mlflow.log_params(hyper_parameters)\n",
    "\n",
    "model = do_train(cfg, model, resume = False)\n",
    "\n",
    "# log pytorch model\n",
    "mlflow.pytorch.log_model(pytorch_model = model,\n",
    "                         artifact_path = 'model',\n",
    "                         conda_env = mlflow.pytorch.get_default_conda_env())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = do_evaluate(cfg, model) # evaluate\n",
    "mlflow.log_metrics(result['segm']) # log evaluation result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare output with groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_gt(dataset_val, cfg, val_name, 'output/model_best.pth')\n",
    "mlflow.log_artifacts('compare_result/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
